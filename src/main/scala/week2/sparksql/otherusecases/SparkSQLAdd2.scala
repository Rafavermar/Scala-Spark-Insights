package week2.sparksql.otherusecases

import org.apache.spark.sql.functions.{avg, col, expr}
import org.apache.spark.sql.{Column, DataFrame}

import scala.Console._

/**
 * This object demonstrates various DataFrame operations in Spark SQL.
 * It illustrates how to create and manipulate DataFrames using Spark's built-in functions.
 */
object SparkSQLAdd2 extends App {

  // Import necessary Spark SQL components
  import org.apache.spark.sql.SparkSession

  /**
   * Create a SparkSession which is the entry point to Spark SQL.
   * It allows the application to communicate with Spark.
   */
  implicit val spark: SparkSession = SparkSession.builder()
    .appName("SparkSQLAdd2") // Name as it appears in the Spark UI
    .master("local[*]") // Use all available cores
    .getOrCreate()

  // Set the log level to "ERROR" to minimize log output
  spark.sparkContext.setLogLevel("ERROR")

  // Print the Spark version
  println(s"Spark version: ${spark.version}")
  println("Created Spark session with the following configuration:")

  // Display all current Spark configuration settings
  val maxKeyLength = spark.conf.getAll.map(_._1.length).max
  spark.conf.getAll.foreach {
    case (k, v) => println(s"${k.padTo(maxKeyLength, ' ')}: $v")
  }
  println()

  // Create a DataFrame from JSON data generated by IotDataGenerator
  val df = spark.read
    .option("inferSchema", "true")
    .json(IoTDataGenerator.IotJsonDataPath)

  df.printSchema()
  df.show(truncate = false)

  // Print the count of records in the DataFrame
  println(df.count())

  // Cache the DataFrame to improve performance for multiple operations
  df.cache()

  // Various DataFrame operations
  println("DataFrame Operations:")
  println("-------------------------------")

  // Print the number of distinct devices
  println("Number of distinct devices:")
  df.select("deviceId").distinct().count()
  df.select("deviceId").distinct().show()
  println()

  // Group by device type and count each group
  println("Number of devices by type:")
  df.groupBy("deviceType").count().show()

  // Filter operations
  println("Devices with temperature greater than 5 degrees:")
  df.filter(col("temperature") > 5).show()

  // Define conditions using columns
  val conditionTemperature: Column = col("temperature") > 5
  val conditionHumidity: Column = col("humidity") > 50

  println("Devices with humidity greater than 50%:")
  df.filter(conditionHumidity).show()

  val conditions = conditionTemperature && conditionHumidity
  println(BOLD + BLUE + "Devices with temperature greater than 5 and humidity greater than 50:" + RESET)
  df.filter(conditions).show()

  // Aggregation operations
  println("Aggregation Operations:")
  println("-------------------------------------")

  println("Average temperature:")
  df.agg("temperature" -> "avg").show()
  df.agg(avg("temperature").alias("average_temperature")).show()

  // Aggregations with alias and expressions
  println("Average temperature with alias and expressions:")
  df.agg(avg("temperature").alias("average_temperature"), avg("humidity").alias("average_humidity")).show()

  // Column operations with expressions, functions, and withColumn / withColumnRenamed
  println("Column operations with expressions, functions and withColumn / withColumnRenamed:")
  println("------------------------------------------------------------------------------------")

  println("Columns with expressions:")
  df.withColumn("temperature_x2", col("temperature") * 2).show()

  println("Columns with functions and alias:")
  df.withColumn("temperature_x2", expr("temperature * 2") as "temperature_x2").show()

  // Dropping and renaming columns
  println(BOLD + RED + "----- Drop and Rename Columns -----" + RESET)
  val dfWithoutTemperature: DataFrame = df.drop("temperature")
  dfWithoutTemperature.show()

  println(BOLD + RED + "----- Drop Multiple Columns -----" + RESET)
  df.drop("temperature", "humidity").show(truncate = false)

  // Stop Spark and exit
  spark.stop()
  System.exit(0)
}
